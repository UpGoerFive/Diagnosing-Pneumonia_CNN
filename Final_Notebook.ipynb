{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y6Ch8wEbwBB8",
        "4XnpfU81jKt5",
        "pMQsksVNjUo7"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNtUkK+v7sEMDIFQswJHeAF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grace-arina/Diagnosing-Pneumonia_CNN/blob/Olgert/Final_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pneumonia"
      ],
      "metadata": {
        "id": "c50_ar43pIQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSo8MBV5Rasz",
        "outputId": "ca7cbac5-f879-4559-f148-7d85bd21d249"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary dependencies \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "import datetime\n",
        "\n",
        "import scipy\n",
        "import os, shutil\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, SeparableConv2D , BatchNormalization,Dropout\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.layers import Rescaling, RandomFlip, RandomRotation, RandomTranslation, RandomZoom\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "sEwUGRo0R8Jq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up models with Tensorflow 2.5\n",
        "Version capped at Tensorflow 2.5 to use local GPU"
      ],
      "metadata": {
        "id": "Y6Ch8wEbwBB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7zhaKZUpwIOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading files and recaling images\n",
        "idg = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_set = idg.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/Diagnosing-Pneumonia_CNN/chest_xray/train',\n",
        "                                                 target_size=(150, 150),\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='binary',\n",
        "                                                 color_mode='grayscale')\n",
        "\n",
        "val_set = idg.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/Diagnosing-Pneumonia_CNN/chest_xray/val',\n",
        "                                          target_size=(150, 150),\n",
        "                                          batch_size=32,\n",
        "                                          class_mode='binary',\n",
        "                                          color_mode='grayscale')\n",
        "\n",
        "test_set = idg.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/Diagnosing-Pneumonia_CNN/chest_xray/test',\n",
        "                                            target_size=(150, 150),\n",
        "                                            batch_size=32,\n",
        "                                            class_mode='binary',\n",
        "                                            color_mode='grayscale')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpv5dwN_SBbs",
        "outputId": "667fee9c-0433-4638-c149-5591e7163e9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5022 images belonging to 2 classes.\n",
            "Found 210 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Baseline Model"
      ],
      "metadata": {
        "id": "4XnpfU81jKt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting paramaters on early stopping\n",
        "earlystop = EarlyStopping(monitor='val_loss',\n",
        "                          patience=20,\n",
        "                          verbose=1,\n",
        "                          mode='min',\n",
        "                          restore_best_weights=True)\n",
        "\n",
        "#If recreating notebook, use the below 3 lines of code to save a checkpoint for your models, where your logs should be saved and the Tensorboard callback to save your models in Tensorboard\n",
        "\n",
        "# mod_checkpt = ModelCheckpoint(filepath =\"/content/drive/MyDrive/Colab Notebooks/Diagnosing-Pneumonia_CNN/Models/my_model1.h5\",monitor= \"val_loss\", save_best_only=True )\n",
        "# log_dir = \"/content/drive/MyDrive/Colab Notebooks/Diagnosing-Pneumonia_CNN/Models/\"\n",
        "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "djTVMLQEak19"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up a baseline model\n",
        "model1 = models.Sequential()\n",
        "\n",
        "#1st Layer\n",
        "model1.add(layers.Conv2D(32, 7, input_shape=(150,150,1), padding='same',activation='relu'))\n",
        "model1.add(layers.MaxPooling2D(2))\n",
        "\n",
        "#2nd layer\n",
        "model1.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "model1.add(layers.MaxPooling2D(2))\n",
        "\n",
        "#Flattening data into 1 vector\n",
        "model1.add(layers.Flatten())\n",
        "model1.add(layers.Dense(64, activation='relu'))\n",
        "model1.add(layers.Dense(32, activation='relu'))\n",
        "model1.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "#Compiling model with adam optemizer, binary_crossentropy since this is binairy classification and setting metrics to accuracy and recall\n",
        "model1.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['acc', tf.metrics.Recall()])\n",
        "model1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgeXGCrmYZfR",
        "outputId": "497c9380-a37e-476c-8dc8-c8d6693c0a77"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 150, 150, 32)      1600      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 75, 75, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 75, 75, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 37, 37, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 87616)             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                5607488   \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,629,697\n",
            "Trainable params: 5,629,697\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting model, with 100 steps and 30 epochs and using early stop when our model stops improving\n",
        "history1 = model1.fit(train_set,\n",
        "                    validation_data=val_set,\n",
        "                    steps_per_epoch=100,\n",
        "                    epochs=30,\n",
        "                    callbacks=[earlystop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A66pSj7vZt5g",
        "outputId": "e01cd633-0eda-4b2a-de73-665896f1de47"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - 49s 480ms/step - loss: 0.2441 - acc: 0.9006 - recall_1: 0.9575 - val_loss: 0.1542 - val_acc: 0.9429 - val_recall_1: 0.9276\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 48s 477ms/step - loss: 0.1079 - acc: 0.9600 - recall_1: 0.9759 - val_loss: 0.0965 - val_acc: 0.9714 - val_recall_1: 0.9671\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 49s 488ms/step - loss: 0.0983 - acc: 0.9669 - recall_1: 0.9775 - val_loss: 0.1228 - val_acc: 0.9429 - val_recall_1: 0.9868\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 49s 484ms/step - loss: 0.0815 - acc: 0.9694 - recall_1: 0.9805 - val_loss: 0.1253 - val_acc: 0.9476 - val_recall_1: 0.9868\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 45s 446ms/step - loss: 0.0702 - acc: 0.9716 - recall_1: 0.9833 - val_loss: 0.1039 - val_acc: 0.9571 - val_recall_1: 0.9474\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 45s 449ms/step - loss: 0.0587 - acc: 0.9791 - recall_1: 0.9864 - val_loss: 0.1009 - val_acc: 0.9524 - val_recall_1: 0.9868\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 45s 451ms/step - loss: 0.0522 - acc: 0.9812 - recall_1: 0.9877 - val_loss: 0.1001 - val_acc: 0.9571 - val_recall_1: 0.9803\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 45s 453ms/step - loss: 0.0524 - acc: 0.9781 - recall_1: 0.9858 - val_loss: 0.1546 - val_acc: 0.9429 - val_recall_1: 0.9868\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 44s 437ms/step - loss: 0.0542 - acc: 0.9800 - recall_1: 0.9865 - val_loss: 0.1014 - val_acc: 0.9524 - val_recall_1: 0.9868\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 45s 446ms/step - loss: 0.0323 - acc: 0.9887 - recall_1: 0.9916 - val_loss: 0.1216 - val_acc: 0.9571 - val_recall_1: 0.9934\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 44s 440ms/step - loss: 0.0460 - acc: 0.9847 - recall_1: 0.9907 - val_loss: 0.0811 - val_acc: 0.9619 - val_recall_1: 0.9803\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 45s 444ms/step - loss: 0.0294 - acc: 0.9897 - recall_1: 0.9929 - val_loss: 0.1731 - val_acc: 0.9286 - val_recall_1: 0.9868\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 45s 449ms/step - loss: 0.0330 - acc: 0.9891 - recall_1: 0.9933 - val_loss: 0.0904 - val_acc: 0.9714 - val_recall_1: 0.9868\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 44s 441ms/step - loss: 0.0244 - acc: 0.9919 - recall_1: 0.9945 - val_loss: 0.1036 - val_acc: 0.9619 - val_recall_1: 0.9803\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 45s 448ms/step - loss: 0.0211 - acc: 0.9925 - recall_1: 0.9946 - val_loss: 0.2432 - val_acc: 0.9286 - val_recall_1: 0.9934\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 46s 458ms/step - loss: 0.0255 - acc: 0.9906 - recall_1: 0.9936 - val_loss: 0.1184 - val_acc: 0.9619 - val_recall_1: 0.9539\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 48s 476ms/step - loss: 0.0196 - acc: 0.9934 - recall_1: 0.9958 - val_loss: 0.0803 - val_acc: 0.9619 - val_recall_1: 0.9671\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 0.0148 - acc: 0.9953 - recall_1: 0.9958 - val_loss: 0.2333 - val_acc: 0.9381 - val_recall_1: 0.9934\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 0.0111 - acc: 0.9962 - recall_1: 0.9970 - val_loss: 0.1152 - val_acc: 0.9762 - val_recall_1: 0.9803\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 46s 457ms/step - loss: 0.0026 - acc: 0.9991 - recall_1: 0.9992 - val_loss: 0.1304 - val_acc: 0.9619 - val_recall_1: 0.9737\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 45s 445ms/step - loss: 0.0046 - acc: 0.9981 - recall_1: 0.9987 - val_loss: 0.1009 - val_acc: 0.9667 - val_recall_1: 0.9868\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 47s 465ms/step - loss: 0.0034 - acc: 0.9994 - recall_1: 0.9996 - val_loss: 0.1720 - val_acc: 0.9571 - val_recall_1: 0.9868\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 50s 498ms/step - loss: 0.0231 - acc: 0.9941 - recall_1: 0.9971 - val_loss: 0.1348 - val_acc: 0.9524 - val_recall_1: 0.9803\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 49s 486ms/step - loss: 0.0072 - acc: 0.9978 - recall_1: 0.9987 - val_loss: 0.1231 - val_acc: 0.9667 - val_recall_1: 0.9868\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 46s 456ms/step - loss: 0.0346 - acc: 0.9866 - recall_1: 0.9903 - val_loss: 0.1721 - val_acc: 0.9381 - val_recall_1: 0.9145\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 46s 456ms/step - loss: 0.0182 - acc: 0.9934 - recall_1: 0.9949 - val_loss: 0.1810 - val_acc: 0.9524 - val_recall_1: 0.9934\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 47s 465ms/step - loss: 0.0054 - acc: 0.9984 - recall_1: 0.9996 - val_loss: 0.1036 - val_acc: 0.9762 - val_recall_1: 0.9737\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 48s 482ms/step - loss: 6.5884e-04 - acc: 1.0000 - recall_1: 1.0000 - val_loss: 0.1231 - val_acc: 0.9619 - val_recall_1: 0.9934\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 50s 499ms/step - loss: 3.7311e-04 - acc: 1.0000 - recall_1: 1.0000 - val_loss: 0.1116 - val_acc: 0.9667 - val_recall_1: 0.9934\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 50s 500ms/step - loss: 1.5885e-04 - acc: 1.0000 - recall_1: 1.0000 - val_loss: 0.1361 - val_acc: 0.9667 - val_recall_1: 0.9934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating model on test data\n",
        "model1.evaluate(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6782JOcjtYT",
        "outputId": "3ca7ee77-f3b9-4e00-ac20-f23c36db5da6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 94s 5s/step - loss: 4.0147 - acc: 0.7308 - recall_1: 0.9949\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.014666557312012, 0.7307692170143127, 0.9948717951774597]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Model With Image Augmentation"
      ],
      "metadata": {
        "id": "pMQsksVNjUo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the same as the best previous model with image augmentation\n",
        "augment_gen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=.2,\n",
        "                                   height_shift_range=.2,\n",
        "                                   shear_range=.2,\n",
        "                                   zoom_range=.2,\n",
        "                                   brightness_range=[.5, 1.5])\n",
        "\n",
        "augment_set = augment_gen.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/Diagnosing-Pneumonia_CNN/chest_xray/train',\n",
        "                                                 target_size=(150, 150),\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='binary',\n",
        "                                                 color_mode='grayscale')\n",
        "\n",
        "augmented = models.Sequential()\n",
        "\n",
        "augmented.add(layers.Conv2D(32, 7, padding='valid', input_shape=(150,150,1), activation='relu'))\n",
        "augmented.add(layers.MaxPooling2D(2))\n",
        "\n",
        "augmented.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "augmented.add(layers.MaxPooling2D(2))\n",
        "\n",
        "augmented.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "augmented.add(layers.MaxPooling2D(3))\n",
        "\n",
        "augmented.add(layers.Flatten())\n",
        "augmented.add(layers.Dense(64, activation='relu'))\n",
        "augmented.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "augmented.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['acc', tf.metrics.Recall()])\n",
        "\n",
        "augmented.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzSZlXDNjR6d",
        "outputId": "a8e53a91-2353-4189-fdbc-b44de1928dcb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5022 images belonging to 2 classes.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 144, 144, 32)      1600      \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 72, 72, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 36, 36, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 36, 36, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 12, 12, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 9216)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                589888    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 646,977\n",
            "Trainable params: 646,977\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "historyaug = augmented.fit(augment_set,\n",
        "                    validation_data=val_set,\n",
        "                    steps_per_epoch=100,\n",
        "                    epochs=30,\n",
        "                    callbacks=[earlystop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzZJ_n4zaedX",
        "outputId": "91367337-f7d9-494e-9643-39907871c111"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - 61s 580ms/step - loss: 0.5348 - acc: 0.7419 - recall_2: 0.9395 - val_loss: 0.4802 - val_acc: 0.7429 - val_recall_2: 0.9539\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 60s 599ms/step - loss: 0.4535 - acc: 0.7649 - recall_2: 0.8573 - val_loss: 0.4251 - val_acc: 0.7381 - val_recall_2: 0.9934\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 57s 569ms/step - loss: 0.4131 - acc: 0.7919 - recall_2: 0.8869 - val_loss: 0.3370 - val_acc: 0.7952 - val_recall_2: 0.9013\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 56s 564ms/step - loss: 0.3737 - acc: 0.8184 - recall_2: 0.9047 - val_loss: 0.3180 - val_acc: 0.8619 - val_recall_2: 0.8553\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 59s 588ms/step - loss: 0.3567 - acc: 0.8469 - recall_2: 0.9099 - val_loss: 0.3050 - val_acc: 0.8333 - val_recall_2: 0.8882\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 59s 589ms/step - loss: 0.3057 - acc: 0.8684 - recall_2: 0.9212 - val_loss: 0.3110 - val_acc: 0.8619 - val_recall_2: 0.8553\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 59s 585ms/step - loss: 0.3086 - acc: 0.8749 - recall_2: 0.9157 - val_loss: 0.2583 - val_acc: 0.8857 - val_recall_2: 0.8684\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 59s 592ms/step - loss: 0.2809 - acc: 0.8793 - recall_2: 0.9239 - val_loss: 0.2103 - val_acc: 0.9238 - val_recall_2: 0.9276\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 59s 591ms/step - loss: 0.2574 - acc: 0.8890 - recall_2: 0.9273 - val_loss: 0.2181 - val_acc: 0.9190 - val_recall_2: 0.9211\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 58s 576ms/step - loss: 0.2666 - acc: 0.8843 - recall_2: 0.9207 - val_loss: 0.2244 - val_acc: 0.9095 - val_recall_2: 0.9671\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 54s 535ms/step - loss: 0.2541 - acc: 0.8906 - recall_2: 0.9243 - val_loss: 0.2221 - val_acc: 0.9238 - val_recall_2: 0.9276\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 52s 515ms/step - loss: 0.2545 - acc: 0.8941 - recall_2: 0.9231 - val_loss: 0.2045 - val_acc: 0.9333 - val_recall_2: 0.9671\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 60s 597ms/step - loss: 0.2500 - acc: 0.8946 - recall_2: 0.9254 - val_loss: 0.1850 - val_acc: 0.9333 - val_recall_2: 0.9474\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 62s 615ms/step - loss: 0.2476 - acc: 0.8969 - recall_2: 0.9251 - val_loss: 0.2516 - val_acc: 0.8857 - val_recall_2: 0.8487\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 50s 500ms/step - loss: 0.2374 - acc: 0.9012 - recall_2: 0.9253 - val_loss: 0.1911 - val_acc: 0.9429 - val_recall_2: 0.9803\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 50s 497ms/step - loss: 0.2397 - acc: 0.8990 - recall_2: 0.9300 - val_loss: 0.1746 - val_acc: 0.9429 - val_recall_2: 0.9474\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 0.2168 - acc: 0.9065 - recall_2: 0.9361 - val_loss: 0.1886 - val_acc: 0.9333 - val_recall_2: 0.9605\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 70s 700ms/step - loss: 0.2178 - acc: 0.9118 - recall_2: 0.9340 - val_loss: 0.1679 - val_acc: 0.9429 - val_recall_2: 0.9605\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 62s 616ms/step - loss: 0.2144 - acc: 0.9124 - recall_2: 0.9398 - val_loss: 0.1766 - val_acc: 0.9429 - val_recall_2: 0.9671\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 57s 566ms/step - loss: 0.2226 - acc: 0.9034 - recall_2: 0.9302 - val_loss: 0.2459 - val_acc: 0.8857 - val_recall_2: 0.8487\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 61s 613ms/step - loss: 0.2126 - acc: 0.9094 - recall_2: 0.9336 - val_loss: 0.1588 - val_acc: 0.9429 - val_recall_2: 0.9605\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 64s 642ms/step - loss: 0.2104 - acc: 0.9096 - recall_2: 0.9346 - val_loss: 0.1576 - val_acc: 0.9476 - val_recall_2: 0.9605\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 66s 656ms/step - loss: 0.2060 - acc: 0.9137 - recall_2: 0.9424 - val_loss: 0.1901 - val_acc: 0.9286 - val_recall_2: 0.9276\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 66s 663ms/step - loss: 0.2018 - acc: 0.9231 - recall_2: 0.9413 - val_loss: 0.1702 - val_acc: 0.9476 - val_recall_2: 0.9868\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 68s 676ms/step - loss: 0.2061 - acc: 0.9124 - recall_2: 0.9396 - val_loss: 0.1819 - val_acc: 0.9238 - val_recall_2: 0.9079\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 67s 673ms/step - loss: 0.2023 - acc: 0.9209 - recall_2: 0.9378 - val_loss: 0.1548 - val_acc: 0.9429 - val_recall_2: 0.9605\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 65s 652ms/step - loss: 0.1894 - acc: 0.9259 - recall_2: 0.9458 - val_loss: 0.1511 - val_acc: 0.9429 - val_recall_2: 0.9408\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 64s 639ms/step - loss: 0.1817 - acc: 0.9303 - recall_2: 0.9501 - val_loss: 0.1722 - val_acc: 0.9429 - val_recall_2: 1.0000\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 64s 637ms/step - loss: 0.1868 - acc: 0.9212 - recall_2: 0.9404 - val_loss: 0.1670 - val_acc: 0.9238 - val_recall_2: 0.9145\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 62s 619ms/step - loss: 0.1929 - acc: 0.9191 - recall_2: 0.9384 - val_loss: 0.1582 - val_acc: 0.9286 - val_recall_2: 0.9408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented.evaluate(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw2OoMlwaehe",
        "outputId": "6584c9a7-7035-426c-e65b-fe450750e3d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 7s 360ms/step - loss: 0.4662 - acc: 0.8317 - recall_2: 0.9282\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.46619439125061035, 0.8317307829856873, 0.928205132484436]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Model with Tensorflow 2.5"
      ],
      "metadata": {
        "id": "xgXfb3ynlp2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Final Model\n",
        "\n",
        "long_run = models.Sequential()\n",
        "\n",
        "long_run.add(layers.Conv2D(32, 7, padding='valid', input_shape=(150,150,1), activation='relu'))\n",
        "long_run.add(layers.MaxPooling2D(2))\n",
        "\n",
        "long_run.add(layers.Conv2D(128, 3, padding='valid', activation='relu'))\n",
        "long_run.add(layers.MaxPooling2D(2))\n",
        "\n",
        "long_run.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "long_run.add(layers.MaxPooling2D(2))\n",
        "\n",
        "long_run.add(layers.Conv2D(64, 2, padding='same', activation='relu'))\n",
        "long_run.add(layers.MaxPooling2D(2))\n",
        "\n",
        "long_run.add(layers.Flatten())\n",
        "long_run.add(layers.Dense(64, activation='relu'))\n",
        "long_run.add(layers.Dense(64, activation='relu'))\n",
        "long_run.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "long_run.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['acc', tf.metrics.Recall()])\n",
        "long_run.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCZQl6UgaekL",
        "outputId": "f751d93e-efa7-49d6-d9e0-52de9d99f3c0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_7 (Conv2D)           (None, 144, 144, 32)      1600      \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 72, 72, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 70, 70, 128)       36992     \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 35, 35, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 35, 35, 64)        73792     \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 17, 17, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 17, 17, 64)        16448     \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 8, 8, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 64)                262208    \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 395,265\n",
            "Trainable params: 395,265\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_long_run = long_run.fit(augment_set,\n",
        "                    validation_data=val_set,\n",
        "                    epochs=200,\n",
        "                    callbacks=[earlystop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQEuN4xEmwWy",
        "outputId": "75290723-f4bb-4cbe-bef8-23b1d4f8378d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "157/157 [==============================] - 91s 579ms/step - loss: 0.5061 - acc: 0.7429 - recall_3: 0.9601 - val_loss: 0.4461 - val_acc: 0.7238 - val_recall_3: 1.0000\n",
            "Epoch 2/200\n",
            "157/157 [==============================] - 89s 566ms/step - loss: 0.4457 - acc: 0.7662 - recall_3: 0.9185 - val_loss: 0.4768 - val_acc: 0.7429 - val_recall_3: 1.0000\n",
            "Epoch 3/200\n",
            "157/157 [==============================] - 94s 595ms/step - loss: 0.3813 - acc: 0.8160 - recall_3: 0.8906 - val_loss: 0.3543 - val_acc: 0.8333 - val_recall_3: 0.8618\n",
            "Epoch 4/200\n",
            "157/157 [==============================] - 90s 572ms/step - loss: 0.3422 - acc: 0.8453 - recall_3: 0.8896 - val_loss: 0.3639 - val_acc: 0.7667 - val_recall_3: 0.9868\n",
            "Epoch 5/200\n",
            "157/157 [==============================] - 93s 590ms/step - loss: 0.3253 - acc: 0.8536 - recall_3: 0.8979 - val_loss: 0.2880 - val_acc: 0.8857 - val_recall_3: 0.9079\n",
            "Epoch 6/200\n",
            "157/157 [==============================] - 93s 589ms/step - loss: 0.3021 - acc: 0.8684 - recall_3: 0.9102 - val_loss: 0.2991 - val_acc: 0.8476 - val_recall_3: 0.8026\n",
            "Epoch 7/200\n",
            "157/157 [==============================] - 90s 572ms/step - loss: 0.3106 - acc: 0.8646 - recall_3: 0.9161 - val_loss: 0.2388 - val_acc: 0.8952 - val_recall_3: 0.8684\n",
            "Epoch 8/200\n",
            "157/157 [==============================] - 88s 562ms/step - loss: 0.2645 - acc: 0.8897 - recall_3: 0.9292 - val_loss: 0.2415 - val_acc: 0.8905 - val_recall_3: 0.8750\n",
            "Epoch 9/200\n",
            "157/157 [==============================] - 88s 558ms/step - loss: 0.2548 - acc: 0.8951 - recall_3: 0.9303 - val_loss: 0.2541 - val_acc: 0.8905 - val_recall_3: 0.8684\n",
            "Epoch 10/200\n",
            "157/157 [==============================] - 89s 563ms/step - loss: 0.2540 - acc: 0.8921 - recall_3: 0.9322 - val_loss: 0.1954 - val_acc: 0.9238 - val_recall_3: 0.9145\n",
            "Epoch 11/200\n",
            "157/157 [==============================] - 88s 561ms/step - loss: 0.2456 - acc: 0.8967 - recall_3: 0.9292 - val_loss: 0.1931 - val_acc: 0.9286 - val_recall_3: 0.9408\n",
            "Epoch 12/200\n",
            " 65/157 [===========>..................] - ETA: 51s - loss: 0.2636 - acc: 0.8903 - recall_3: 0.9215"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "long_run.evaluate(test_set)"
      ],
      "metadata": {
        "id": "JL1PcCeimxod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models with updated Tensor 2.8"
      ],
      "metadata": {
        "id": "1KHnW47ym5Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Similar to above, setting image size and batch size\n",
        "image_size = (150,150)\n",
        "batch_size = 50\n",
        "\n",
        "#Loading images from directory\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/Diagnosing-Pneumonia_CNN/chest_xray/train\",\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    color_mode=\"grayscale\")\n",
        "\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/Diagnosing-Pneumonia_CNN/chest_xray/test\",\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    color_mode=\"grayscale\")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/Diagnosing-Pneumonia_CNN/chest_xray/val\",\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    color_mode=\"grayscale\")\n"
      ],
      "metadata": {
        "id": "ULDv4MOxm_oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Class names\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "QIfORtKZnHPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking size of imagees and labels\n",
        "for image_batch, labels_batch in train_ds.take(1):\n",
        "  print(image_batch.shape)\n",
        "  print(f\"Labels for train ds: {labels_batch}\")\n",
        "  break"
      ],
      "metadata": {
        "id": "5uwp6UP4nScK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pulls labels from test_ds\n",
        "test_labels= np.concatenate([y for x, y in test_ds], axis=0)"
      ],
      "metadata": {
        "id": "qF4yzEDZnXfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating function to evaluate our models\n",
        "\n",
        "def model_eval(model):#input current model, train data and test data\n",
        "  results_train = model.evaluate(train_ds) #evaluates train input\n",
        "  results_test = model.evaluate(test_ds) #evaluates test input\n",
        "  pred = model.predict(test_ds)\n",
        "  preds = np.where(pred>0.5, 1,0)#if probability is less than .5 assign 0 if greater than 0 assign 1\n",
        "  cm  = confusion_matrix(test_labels, preds, normalize=\"true\")\n",
        "  return print(f'Train Loss: {results_train[0]}, Train Accuracy: {results_train[1]} \\n Test Loss: {results_test[0]}, Test Accuracy: {results_test[1]}.')"
      ],
      "metadata": {
        "id": "2GXoMfJInlDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting autotune to pre-fetch train and validation data\n",
        "tpain = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.prefetch(buffer_size=tpain)\n",
        "val_ds = val_ds.prefetch(buffer_size=tpain)"
      ],
      "metadata": {
        "id": "fB-1ksr-nY00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating 1st model with SeparableConv2D"
      ],
      "metadata": {
        "id": "GMlb4jf4oyAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_model(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    # Image augmentation block\n",
        "    x = layers.Rescaling(1./255)(inputs)\n",
        "    x = layers.RandomRotation(0.6)(x)\n",
        "    x = layers.RandomZoom(0.2)(x)\n",
        "\n",
        "    x = Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "    x = Conv2D(32, 3, activation='relu', padding='same')(x)    \n",
        "    x = MaxPooling2D((2,2), name='pool1')(x)\n",
        "\n",
        "#Introducing first SeparableConv2D and BatchNormalization to our modeling\n",
        "    x = layers.SeparableConv2D(64, 3, padding=\"same\", activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x) \n",
        "    x = MaxPooling2D((2,2), name='pool2')(x)\n",
        "\n",
        "    x = Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool3')(x)\n",
        "\n",
        "\n",
        "    x = Flatten()(x)   \n",
        "    x = Dense(256, activation='relu')(x)   \n",
        "    x = Dense(128, activation='relu')(x)\n",
        "\n",
        "    if num_classes == 2:\n",
        "        activation = \"sigmoid\"\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = \"softmax\"\n",
        "        units = num_classes\n",
        "\n",
        "    outputs = layers.Dense(units, activation=activation)(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "model3 = make_model(input_shape=(image_size) + (1,), num_classes=2)\n",
        "#Showing the models steps\n",
        "keras.utils.plot_model(model3, show_shapes=True)"
      ],
      "metadata": {
        "id": "ORSfJFe5nhaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.summary()"
      ],
      "metadata": {
        "id": "1BFsys7spVSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model checkpoint monitor val_loss and save best model\n",
        "#Setting log directory and using tensoboard callback so our data can be uploaded to Tensorboard \n",
        "#Similar to above, uncoment the below rows to save checkpoints and load data to TensorBoard\n",
        "\n",
        "# mod_checkpt = ModelCheckpoint(filepath =\"/content/drive/MyDrive/Colab Notebooks/Diagnosing-Pneumonia_CNN/Models/my_model3.h5\",monitor= \"val_loss\", save_best_only=True )\n",
        "#log_dir = \"/content/drive/MyDrive/Colab Notebooks/Diagnosing-Pneumonia_CNN/Models/logs fit\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "#Using early stop to stop model after 10 epochs of no improvenment\n",
        "early_stop = EarlyStopping(monitor= \"val_loss\", mode=\"min\", patience = 10, restore_best_weights=True)\n",
        "lr_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=5, factor=.000000001)\n",
        "callbacks = [early_stop, lr_rate]"
      ],
      "metadata": {
        "id": "TURHECJZpVVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\", tf.metrics.Recall()])\n",
        "\n",
        "result3 = model3.fit(train_ds, epochs=500, batch_size=50, validation_data=val_ds, shuffle=True, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "sliuDOPZqb1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3_eval = model_eval(model3)"
      ],
      "metadata": {
        "id": "2Y1Zrxr9qhWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second model with SeparableConv2D"
      ],
      "metadata": {
        "id": "ndSxpl7-qsGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_model(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    # Image augmentation block\n",
        "    x = layers.Rescaling(1./255)(inputs)\n",
        "    x = layers.RandomRotation(0.6)(x)\n",
        "    x = layers.RandomZoom(0.2)(x)\n",
        "\n",
        "    x = Conv2D(32, 3, activation='relu', padding='same')(x)    \n",
        "    x = MaxPooling2D((2,2), name='pool1')(x)\n",
        "\n",
        "#Introducing first SeparableConv2D and BatchNormalization to our modeling\n",
        "    x = layers.SeparableConv2D(64, 3, padding=\"same\", activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x) \n",
        "    x = MaxPooling2D((2,2), name='pool2')(x)\n",
        "\n",
        "    x = Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool3')(x)\n",
        "\n",
        "    x = layers.SeparableConv2D(64, 3, padding=\"same\", activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x) \n",
        "    x = MaxPooling2D((2,2), name='pool2')(x)\n",
        "\n",
        "    x = Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool4')(x)\n",
        "\n",
        "    x = Flatten()(x)   \n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)    \n",
        "    x = Dense(128, activation='relu')(x)\n",
        "\n",
        "    if num_classes == 2:\n",
        "        activation = \"sigmoid\"\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = \"softmax\"\n",
        "        units = num_classes\n",
        "\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(units, activation=activation)(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "model4 = make_model(input_shape=(image_size) + (1,), num_classes=2)\n",
        "#Showing the models steps\n",
        "keras.utils.plot_model(model4, show_shapes=True)"
      ],
      "metadata": {
        "id": "xjOsU6vIqkkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.summary()"
      ],
      "metadata": {
        "id": "NoDFHGtDrc0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.compile(optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\", tf.metrics.Recall()])\n",
        "\n",
        "result4 = model4.fit(train_ds, epochs=500, batch_size=50, validation_data=val_ds, shuffle=True, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "wmtjA3XOtzRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4_eval = model_eval(model4)"
      ],
      "metadata": {
        "id": "UZxNHBXutzT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complex model with SeparableConv2D, BatchNormalization, SeparableConv2D loop with more sizes and residuals"
      ],
      "metadata": {
        "id": "SBHTtz-zsa-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    # Image augmentation block\n",
        "    x = layers.RandomRotation(0.6)(x)\n",
        "    x = layers.RandomZoom(0.3)(x)\n",
        "\n",
        "\n",
        "    x = layers.Rescaling(1./255)(inputs)\n",
        "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool1')(x)\n",
        "\n",
        "    x = Conv2D(128, 3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool2')(x)\n",
        "\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "    for size in [128, 256, 512]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation)\n",
        "        \n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "\n",
        "    x = Flatten()(x)   \n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x) \n",
        "    x = Dense(64, activation='relu')(x)   \n",
        "\n",
        "    if num_classes == 2:\n",
        "        activation = \"sigmoid\"\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = \"softmax\"\n",
        "        units = num_classes\n",
        "\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(units, activation=activation)(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "model5 = make_model(input_shape=(image_size) + (1,), num_classes=2)\n",
        "keras.utils.plot_model(model5, show_shapes=True)"
      ],
      "metadata": {
        "id": "Zy9UNeitrikA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model5.summary()"
      ],
      "metadata": {
        "id": "HgX4k3vSt68Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model5.compile(optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\", tf.metrics.Recall()])\n",
        "\n",
        "result1 = model5.fit(train_ds, epochs=500, batch_size=50, validation_data=val_ds, shuffle=True, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "dPuWTo0KuWWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding SeparableConv2D is not improving our models, its adding paramaters with no gains, reverting to using Conv2d to keep paramaters low and improve processing speed"
      ],
      "metadata": {
        "id": "L_jgXOixu0br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv2D with Tensor 2.8"
      ],
      "metadata": {
        "id": "Xbz7uj8vuieL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    # Image augmentation block\n",
        "    x = layers.Rescaling(1./255)(inputs)\n",
        "    x = layers.RandomRotation(0.6)(x)\n",
        "    x = layers.RandomZoom(0.2)(x)\n",
        "\n",
        "    x = Conv2D(32, 3, activation='relu', padding='same')(x)    \n",
        "    x = MaxPooling2D((2,2), name='pool1')(x)\n",
        "\n",
        "    x = Conv2D(64, 3, activation='relu', padding='same')(x)    \n",
        "    x = MaxPooling2D((2,2), name='pool2')(x)    \n",
        "\n",
        "    x = Conv2D(128, 3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool3')(x)\n",
        "\n",
        "    x = Conv2D(256, 3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool4')(x)\n",
        "\n",
        "    x = Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool5')(x)    \n",
        "\n",
        "    x = Flatten()(x)   \n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)    \n",
        "    x = Dense(128, activation='relu')(x)\n",
        "\n",
        "    if num_classes == 2:\n",
        "        activation = \"sigmoid\"\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = \"softmax\"\n",
        "        units = num_classes\n",
        "\n",
        "    outputs = layers.Dense(units, activation=activation)(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "model7 = make_model(input_shape=(image_size) + (1,), num_classes=2)\n",
        "keras.utils.plot_model(model7, show_shapes=True)"
      ],
      "metadata": {
        "id": "wZ443uajvByx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model7.summary()"
      ],
      "metadata": {
        "id": "F7iEXufMvEur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model7.compile(optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\", tf.metrics.Recall()])\n",
        "\n",
        "result_7 = model7.fit(train_ds, epochs=500, batch_size=50, validation_data=val_ds, shuffle=True, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "UzhDQs22vJ86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model7_eval = model_eval(model7)"
      ],
      "metadata": {
        "id": "ER20iVxKvLJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To see a more iterative process please use the notebooks in \"modeling-notebooks\""
      ],
      "metadata": {
        "id": "QsVcPXP4y1GY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion the model that performed best was the 4 Layer Conv2D nerural network. It produced the best results with low paramaters which will make our model faster."
      ],
      "metadata": {
        "id": "rVGJQ5gnzDvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bePc72wMy_iA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}